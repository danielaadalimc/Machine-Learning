# -*- coding: utf-8 -*-
"""ENCODER de ML Clasificadores

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rgMd6aZtTIA_ACv61Vx6sSJQiYNEAp1U
"""

import pandas as pd
from sklearn import datasets
import numpy as np 

# Se carga el conjunto de datos
dfDatos=pd.read_csv(r'DATA2.csv')
dfDatos = pd.DataFrame(dfDatos)

dfDatos = dfDatos.replace(np.nan, "NA")

#Vamos a codificar las respuestas 
from sklearn.preprocessing import LabelEncoder
for col in dfDatos.columns:
  le = LabelEncoder()
  dfDatos[col] = le.fit_transform(dfDatos[col])
dfDatos

data = dfDatos.drop(["NOMBRE",	"APELLIDO1 ",	"APELLIDO2", "GRUPO", "MATRICULA"], axis = 1)
data

import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
import seaborn as sns

sns.set_palette("deep", desat=.6)
sns.set_context(rc={"figure.figsize":(8,4)})

cuenta, cajas, ignorar = plt.hist(data,2)
plt.ylabel('frequencia')
plt.xlabel('valores')
plt.title('Datos originales')
plt.show()

from sklearn.feature_selection import chi2
X_ = data.drop(columns = ["Titulado"], axis=1)
Y_ = data["Titulado"]

chi_scores= chi2(X_,Y_)

#Obtención de variables relevantes
chi_values_h = pd.Series(chi_scores[0], index=X_.columns)
chi_values_h.sort_values(ascending=False, inplace=True)
chi_values_h.plot.bar()

#Top 20 de variables seleccionadas
chi_values_h[0:24]

#Variables con menor relevancia
chi_values_l = pd.Series(chi_scores[1], index=X_.columns)
chi_values_l.sort_values(ascending=False, inplace=True)
chi_values_l.plot.bar()

#Top 20 de variables menos relevantes
chi_values_l[0:24]

"""Evaluación de las variables con los clasificadores seleccionados"""

#Selección de las 20 variables con mayor relevancia 
variables_sel = chi_values_h[0:20]
columnas = list(variables_sel.index)

#Comienzo a llenar mi target y mi data 
dataX = data[columnas]
dataX.shape
dataY = data['Titulado']
X = np.array(dataX)
Y = np.array(dataY)

"""Grid Search para la selección de mejores hiperparámetros"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, stratify=Y)

from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier()

from sklearn.model_selection import GridSearchCV
#Diccionario de hiperparámetros para su busqueda
param_grid = {
    'bootstrap': [True],
    'max_depth': [10, 20, 30, 40],
    'max_features': [2, 3],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [10, 20, 30, 40]
}

grid_search = GridSearchCV(estimator = classifier,
                           param_grid = param_grid, 
                           scoring='accuracy',
                           cv = 5, verbose = 2)

grid_search.fit(X_train,y_train)

grid_search.best_params_

grid_search.best_score_

"""Evaluación del modelo

**Random Forest**
"""

#Selección de clasificador Random Forest
from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(bootstrap=True,
                                  max_depth= 20,
                                  max_features= 3,
                                  min_samples_leaf= 3,
                                  min_samples_split= 8,
                                  n_estimators= 40)

from sklearn import preprocessing
from sklearn.metrics import accuracy_score
import numpy as np

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, stratify=Y)

sns.set_palette("deep", desat=.6)
sns.set_context(rc={"figure.figsize":(8,4)})

cuenta, cajas, ignorar = plt.hist(X_train,2)
plt.ylabel('frequencia')
plt.xlabel('valores')
plt.title('Datos originales')
plt.show()

from sklearn import preprocessing
from sklearn.metrics import accuracy_score
import numpy as np

from sklearn.model_selection import train_test_split
N_REPETITIONS=50

acc=[]

for i in range(N_REPETITIONS):
  #Se segmentan los datos en 80% entrenamiento y 20% para validación
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, stratify=Y)

  # Se normalizan los datos
  min_max_scaler = preprocessing.MinMaxScaler()
  X_train = min_max_scaler.fit_transform(X_train)
  X_test = min_max_scaler.transform(X_test)
  #sc = preprocessing.StandardScaler()
  #X_train = sc.fit_transform(X_train)
  #X_test = sc.transform(X_test)

  # Se entrena el clasificador
  classifier.fit(X_train, y_train)

  # Se evalua el modelo
  y_pred=classifier.predict(X_test)

  #print(classifier.predict([[1,1,2,2,1,1,1,2,1]])) #1
  #print(classifier.predict([[1,1,2,1,2,2,2,2,0]])) #0
  #print(y_pred)
  #print(y_test)

  # Se encuentra la precision (accuracy)
  acc.append(accuracy_score(y_test, y_pred))

print(np.mean(np.array(acc)),'+/-',np.std(np.array(acc)))

sns.set_palette("deep", desat=.6)
sns.set_context(rc={"figure.figsize":(8,4)})

cuenta, cajas, ignorar = plt.hist(X_train,2)
plt.ylabel('frequencia')
plt.xlabel('valores')
plt.title('Datos normalizados ')
plt.show()

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

y_pred=classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred, normalize='true')
cm_display = ConfusionMatrixDisplay(cm)
cm_display.plot()

"""**Árbol de decisión**

Grid Search para la selección de mejores hiperparámetros
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, stratify=Y)

#Selección de clasificador árbol e decisión
from sklearn.tree import DecisionTreeClassifier 
classifier = DecisionTreeClassifier()

from sklearn.model_selection import GridSearchCV
#Diccionario de hiperparámetros para su busqueda
n_components = list(range(1,X.shape[1]+1,1))
param_grid = {
    'criterion' : ['gini', 'entropy'],
    'max_depth' : [10,15,20,25,30,35],
    #'min_samples_split' : range(1,30),
    #'min_samples_leaf' : range(1,30),
    'max_leaf_nodes' :  range(1,30),
    'min_samples_split' : range(1,30)
}

grid_search = GridSearchCV(estimator = classifier,
                           param_grid = param_grid, 
                           scoring='accuracy',
                           cv = 5, verbose = 2)

grid_search.fit(X_train,y_train)

grid_search.best_params_

grid_search.best_score_

#Selección de clasificador árbol e decisión
from sklearn.tree import DecisionTreeClassifier # árbol de decisión para clasificación
from sklearn.model_selection import train_test_split
classifier = DecisionTreeClassifier(max_depth= 10, max_leaf_nodes = 7, min_samples_split = 2 ) # vamos a usar un árbol de profundidad 2

for i in range(N_REPETITIONS):
  #Se segmentan los datos en 80% entrenamiento y 20% para validación
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, stratify=Y)

  # Se normalizan los datos
  min_max_scaler = preprocessing.MinMaxScaler()
  X_train = min_max_scaler.fit_transform(X_train)
  X_test = min_max_scaler.transform(X_test)
  #sc = preprocessing.StandardScaler()
  #X_train = sc.fit_transform(X_train)
  #X_test = sc.transform(X_test)

  # Se entrena el clasificador
  classifier.fit(X_train, y_train)

  # Se evalua el modelo
  y_pred=classifier.predict(X_test)

  #print(classifier.predict([[1,1,2,2,1,1,1,2,1]])) #1
  #print(classifier.predict([[1,1,2,1,2,2,2,2,0]])) #0
  #print(y_pred)
  #print(y_test)

  # Se encuentra la precision (accuracy)
  acc.append(accuracy_score(y_test, y_pred))

print(np.mean(np.array(acc)),'+/-',np.std(np.array(acc)))

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

y_pred=classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred, normalize='true')
cm_display = ConfusionMatrixDisplay(cm)
cm_display.plot()

"""**Support Vector Machine**

Grid Search para la selección de mejores hiperparámetros
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, stratify=Y)

from sklearn.svm import SVC
classifier=SVC()

from sklearn.model_selection import GridSearchCV
#Diccionario de hiperparámetros para su busqueda
param_grid = {
    'gamma':[1,0.1,0.001], 
    'kernel':['linear','rbf'],
    'degree':[2, 4, 8, 12, 16]
}

grid_search = GridSearchCV(estimator = classifier,
                           param_grid = param_grid, 
                           scoring='accuracy',
                           cv = 5, verbose = 4)

grid_search.fit(X_train,y_train)

grid_search.best_params_

grid_search.best_score_

#Selección de clasificador SVC
from sklearn.svm import SVC
classifier=SVC(kernel='linear', degree=2, gamma = 1)

from sklearn import preprocessing
from sklearn.metrics import accuracy_score
import numpy as np

from sklearn.model_selection import train_test_split
N_REPETITIONS=50

acc=[]
for i in range(N_REPETITIONS):
  #Se segmentan los datos en 80% entrenamiento y 20% para validación
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, stratify=Y)

  # Se normalizan los datos
  min_max_scaler = preprocessing.MinMaxScaler()
  X_train = min_max_scaler.fit_transform(X_train)
  X_test = min_max_scaler.transform(X_test)
  #sc = preprocessing.StandardScaler()
  #X_train = sc.fit_transform(X_train)
  #X_test = sc.transform(X_test)

  # Se entrena el clasificador
  classifier.fit(X_train, y_train)

  # Se evalua el modelo
  y_pred=classifier.predict(X_test)

  #print(classifier.predict([[1,1,2,2,1,1,1,2,1]])) #1
  #print(classifier.predict([[1,1,2,1,2,2,2,2,0]])) #0
  #print(y_pred)
  #print(y_test)

  # Se encuentra la precision (accuracy)
  acc.append(accuracy_score(y_test, y_pred))

print(np.mean(np.array(acc)),'+/-',np.std(np.array(acc)))

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

y_pred=classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred, normalize='true')
cm_display = ConfusionMatrixDisplay(cm)
cm_display.plot()